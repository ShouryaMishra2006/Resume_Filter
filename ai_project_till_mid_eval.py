# -*- coding: utf-8 -*-
"""Ai_Project_Till_Mid_Eval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cR_LlwwdhkvgUWgPiOu7tpBLx_oC7b5X

# **Dataset Loading**
"""

!pip install --upgrade kagglehub

import kagglehub
path = kagglehub.dataset_download("snehaanbhawal/resume-dataset")
print("Path to dataset files:", path)

import os

print("Dataset files:", os.listdir(path))

import pandas as pd

csv_file = os.path.join(path, "Resume/Resume.csv")
df = pd.read_csv(csv_file)

print(df.head())

"""# **Dataset Cleaning**"""

import re
import string
import spacy
import nltk
from nltk.corpus import stopwords
from bs4 import BeautifulSoup

nltk.download("stopwords")
nlp = spacy.load("en_core_web_sm")

print(df.isnull().sum())
df.dropna(subset=["Resume_str", "Category"], inplace=True)

def remove_html(text):
    return BeautifulSoup(text, "html.parser").get_text()

df["Resume_str"] = df["Resume_str"].apply(remove_html)

def clean_text(text):
    text = text.lower()  #lowercase conversion
    text = re.sub(r"\d+", "", text) #numbers
    text = re.sub(r"[^\w\s]", "", text)  #special characters
    text = re.sub(r"\s+", " ", text).strip()  #extra spaces
    return text

df["Resume_str"] = df["Resume_str"].apply(clean_text)

stop_words = set(stopwords.words("english"))

def remove_stopwords(text):
    words = text.split()
    words = [word for word in words if word not in stop_words]
    return " ".join(words)

df["Resume_str"] = df["Resume_str"].apply(remove_stopwords)

def lemmatize_text(text):
    doc = nlp(text)
    return " ".join([token.lemma_ for token in doc])

df["Resume_str"] = df["Resume_str"].apply(lemmatize_text)

df.drop_duplicates(subset=["Resume_str"], inplace=True)

df.to_csv("cleaned_resume_dataset.csv", index=False)

print(df.head())

"""# **Data Augmentation**"""

import random
import nltk
import pandas as pd
from nltk.corpus import wordnet
from transformers import pipeline

nltk.download('wordnet')
nltk.download('omw-1.4')

paraphrase = pipeline("text2text-generation", model="humarin/chatgpt_paraphraser_on_T5_base")

def synonym_replacement(sentence, n=1):
    words = sentence.split()
    if not words:
        return sentence
    new_words = words.copy()
    for _ in range(n):
        word = random.choice(words)
        synonyms = wordnet.synsets(word)
        if synonyms:
            synonym = synonyms[0].lemmas()[0].name()
            new_words[new_words.index(word)] = synonym
    return ' '.join(new_words)

df["synonym_replacement"] = df["Resume_str"].apply(lambda x: synonym_replacement(x))
df.to_csv("augmented_resume_dataset.csv", index=False)
print(df.head())

"""# ** Data Vectorization **"""

!pip install gensim

!pip install --upgrade numpy

!pip install numpy

!pip uninstall gensim -y
!pip install gensim

from gensim.models import Word2Vec

word2vec_model = Word2Vec(sentences=df["synonym_replacement"], vector_size=100, window=5, min_count=1, workers=4)

def get_word2vec_vector(tokens):
    vectors = [word2vec_model.wv[word] for word in tokens if word in word2vec_model.wv]
    return sum(vectors) / len(vectors) if vectors else [0] * 100

df["word2vec_vector"] = df["synonym_replacement"].apply(get_word2vec_vector)

print(df.head())

from sklearn.feature_extraction.text import TfidfVectorizer

df["processed_text_tf-idf"] = df["synonym_replacement"].apply(lambda x: " ".join(x))

tfidf_vectorizer = TfidfVectorizer(max_features=5000)
tfidf_vectors = tfidf_vectorizer.fit_transform(df["synonym_replacement"])

tfidf_df = pd.DataFrame(tfidf_vectors.toarray(), columns=tfidf_vectorizer.get_feature_names_out())

df = pd.concat([df, tfidf_df], axis=1)

print(df.head())

df.to_csv("vectorized_resume_dataset.csv", index=False)

"""# **Pretesting with KNN**"""

from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer
import pandas as pd

X = df.drop(columns=['Category', 'word2vec_vector'])
y = df['Category']
scaler = StandardScaler()
encoder = LabelEncoder()
y = encoder.fit_transform(y)
for column in X.select_dtypes(include=['object']).columns:
    X[column] = encoder.fit_transform(X[column])

imputer = SimpleImputer(strategy='mean')
X = imputer.fit_transform(X)
X_scaled = scaler.fit_transform(X)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

knn = KNeighborsClassifier(n_neighbors=5, metric='cosine')

knn.fit(X_train, y_train)

y_pred = knn.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f"KNN Accuracy: {accuracy:.4f}")
print("Classification Report:\n", classification_report(y_test, y_pred))

print(knn.predict(X_train[0].reshape(1, -1)), y_test[0])

print(df.head())

"""# Testing with Other Traditional **ML Models**"""

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

# Logistic Regression
logreg = LogisticRegression(max_iter=1000)
logreg.fit(X_train, y_train)
y_pred_logreg = logreg.predict(X_test)
accuracy_logreg = accuracy_score(y_test, y_pred_logreg)
print(f"Logistic Regression Accuracy: {accuracy_logreg:.4f}")
print("Logistic Regression Classification Report:\n", classification_report(y_test, y_pred_logreg))

print(knn.predict(X_train[0].reshape(1, -1)), y_test[0])

# Decision Tree
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)
y_pred_dt = dt.predict(X_test)
accuracy_dt = accuracy_score(y_test, y_pred_dt)
print(f"Decision Tree Accuracy: {accuracy_dt:.4f}")
print("Decision Tree Classification Report:\n", classification_report(y_test, y_pred_dt))

print(knn.predict(X_train[0].reshape(1, -1)), y_test[0])

# Random Forest
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
accuracy_rf = accuracy_score(y_test, y_pred_rf)
print(f"Random Forest Accuracy: {accuracy_rf:.4f}")
print("Random Forest Classification Report:\n", classification_report(y_test, y_pred_rf))

print(knn.predict(X_train[0].reshape(1, -1)), y_test[0])

# Support Vector Machine
svm = SVC()
svm.fit(X_train, y_train)
y_pred_svm = svm.predict(X_test)
accuracy_svm = accuracy_score(y_test, y_pred_svm)
print(f"SVM Accuracy: {accuracy_svm:.4f}")
print("SVM Classification Report:\n", classification_report(y_test, y_pred_svm))

print(knn.predict(X_train[0].reshape(1, -1)), y_test[0])

print(df.head())

non_numeric_cols = df.select_dtypes(exclude=['number']).columns
print("Non-numeric columns:", non_numeric_cols)

from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()
df['Category'] = encoder.fit_transform(df['Category'])

print(df.head())

df['Category'].unique()

pip install transformers datasets torch scikit-learn pandas numpy

"""# **Training Using Pretrained Embeddings Bert**"""

import pandas as pd
import torch
from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import DataLoader, Dataset
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from torch.optim import AdamW
from transformers import get_scheduler
import torch.nn.functional as F
from tqdm import tqdm

df = df[['Resume_str', 'Category']]

label_encoder = LabelEncoder()
df['Category'] = label_encoder.fit_transform(df['Category'])

train_texts, val_texts, train_labels, val_labels = train_test_split(
    df['Resume_str'].astype(str).tolist(),
    df['Category'].tolist(),
    test_size=0.2,
    random_state=42
)

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)
val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)

class ResumeDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item
train_dataset = ResumeDataset(train_encodings, train_labels)
val_dataset = ResumeDataset(val_encodings, val_labels)

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=len(df['Category'].unique()))
model.to(device)
for param in model.bert.parameters():
    param.requires_grad = False

for param in model.bert.encoder.layer[-3:].parameters():
    param.requires_grad = True

optimizer = AdamW(model.parameters(), lr=5e-5)
num_training_steps = len(train_loader) * 3
lr_scheduler = get_scheduler("linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)

epochs = 3

for epoch in range(epochs):
    model.train()
    loop = tqdm(train_loader, leave=True)
    for batch in loop:
        batch = {key: val.to(device) for key, val in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()

        loop.set_description(f"Epoch {epoch}")
        loop.set_postfix(loss=loss.item())

model.eval()
total, correct = 0, 0

for batch in val_loader:
    batch = {key: val.to(device) for key, val in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)

    predictions = torch.argmax(outputs.logits, dim=-1)
    correct += (predictions == batch["labels"]).sum().item()
    total += batch["labels"].size(0)

accuracy = correct / total
print(f"Validation Accuracy: {accuracy * 100:.2f}%")

sample_text = "Highly skilled software engineer with 5+ years of experience in full-stack development"

inputs = tokenizer(sample_text, padding=True, truncation=True, return_tensors="pt", max_length=512)

inputs = {key: val.to(device) for key, val in inputs.items()}

with torch.no_grad():
    outputs = model(**inputs)
predicted_class = torch.argmax(outputs.logits, dim=1).item()

print(f"Predicted Category: {predicted_class}")

original_category = label_encoder.inverse_transform([predicted_class])[0]

print(f"Predicted Category (decoded): {original_category}")
#Sample Classification Check
sample_text = "Highly skilled software engineer with 5+ years of experience in full-stack development"

inputs = tokenizer(sample_text, padding=True, truncation=True, return_tensors="pt", max_length=512)

inputs = {key: val.to(device) for key, val in inputs.items()}
with torch.no_grad():
    outputs = model(**inputs)
predicted_class = torch.argmax(outputs.logits, dim=1).item()

print(f"Predicted Category: {predicted_class}")
original_category = label_encoder.inverse_transform([predicted_class])[0]

print(f"Predicted Category (decoded): {original_category}")
sample_text = "Highly Experienced in Business and sales , Lead more than 2 companies "

inputs = tokenizer(sample_text, padding=True, truncation=True, return_tensors="pt", max_length=512)

inputs = {key: val.to(device) for key, val in inputs.items()}
with torch.no_grad():
    outputs = model(**inputs)
predicted_class = torch.argmax(outputs.logits, dim=1).item()

print(f"Predicted Category: {predicted_class}")
original_category = label_encoder.inverse_transform([predicted_class])[0]

print(f"Predicted Category (decoded): {original_category}
sample_text = "Chartant Accountant "

inputs = tokenizer(sample_text, padding=True, truncation=True, return_tensors="pt", max_length=512)

inputs = {key: val.to(device) for key, val in inputs.items()}
with torch.no_grad():
    outputs = model(**inputs)
predicted_class = torch.argmax(outputs.logits, dim=1).item()

print(f"Predicted Category: {predicted_class}")
original_category = label_encoder.inverse_transform([predicted_class])[0]

print(f"Predicted Category (decoded): {original_category}
