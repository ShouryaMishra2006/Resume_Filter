# -*- coding: utf-8 -*-
"""AI_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cR_LlwwdhkvgUWgPiOu7tpBLx_oC7b5X

# **Dataset Loading**
"""

!pip install --upgrade kagglehub

import kagglehub
path = kagglehub.dataset_download("snehaanbhawal/resume-dataset")
print("Path to dataset files:", path)

import os

print("Dataset files:", os.listdir(path))

import pandas as pd

csv_file = os.path.join(path, "Resume/Resume.csv")
df = pd.read_csv(csv_file)

print(df.head())

"""# **Dataset Cleaning**"""

import re
import string
import spacy
import nltk
from nltk.corpus import stopwords
from bs4 import BeautifulSoup

nltk.download("stopwords")
nlp = spacy.load("en_core_web_sm")

print(df.isnull().sum())
df.dropna(subset=["Resume_str", "Category"], inplace=True)

def remove_html(text):
    return BeautifulSoup(text, "html.parser").get_text()

df["Resume_str"] = df["Resume_str"].apply(remove_html)

def clean_text(text):
    text = text.lower()  #lowercase conversion
    text = re.sub(r"\d+", "", text) #numbers
    text = re.sub(r"[^\w\s]", "", text)  #special characters
    text = re.sub(r"\s+", " ", text).strip()  #extra spaces
    return text

df["Resume_str"] = df["Resume_str"].apply(clean_text)

stop_words = set(stopwords.words("english"))

def remove_stopwords(text):
    words = text.split()
    words = [word for word in words if word not in stop_words]
    return " ".join(words)

df["Resume_str"] = df["Resume_str"].apply(remove_stopwords)

def lemmatize_text(text):
    doc = nlp(text)
    return " ".join([token.lemma_ for token in doc])

df["Resume_str"] = df["Resume_str"].apply(lemmatize_text)

df.drop_duplicates(subset=["Resume_str"], inplace=True)

df.to_csv("cleaned_resume_dataset.csv", index=False)

print(df.head())