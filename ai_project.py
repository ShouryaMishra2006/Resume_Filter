# -*- coding: utf-8 -*-
"""AI_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cR_LlwwdhkvgUWgPiOu7tpBLx_oC7b5X

# **Dataset Loading**
"""

!pip install --upgrade kagglehub

import kagglehub
path = kagglehub.dataset_download("snehaanbhawal/resume-dataset")
print("Path to dataset files:", path)

import os

print("Dataset files:", os.listdir(path))

import pandas as pd

csv_file = os.path.join(path, "Resume/Resume.csv")
df = pd.read_csv(csv_file)

print(df.head())

"""# **Dataset Cleaning**"""

import re
import string
import spacy
import nltk
from nltk.corpus import stopwords
from bs4 import BeautifulSoup

nltk.download("stopwords")
nlp = spacy.load("en_core_web_sm")

print(df.isnull().sum())
df.dropna(subset=["Resume_str", "Category"], inplace=True)

def remove_html(text):
    return BeautifulSoup(text, "html.parser").get_text()

df["Resume_str"] = df["Resume_str"].apply(remove_html)

def clean_text(text):
    text = text.lower()  #lowercase conversion
    text = re.sub(r"\d+", "", text) #numbers
    text = re.sub(r"[^\w\s]", "", text)  #special characters
    text = re.sub(r"\s+", " ", text).strip()  #extra spaces
    return text

df["Resume_str"] = df["Resume_str"].apply(clean_text)

stop_words = set(stopwords.words("english"))

def remove_stopwords(text):
    words = text.split()
    words = [word for word in words if word not in stop_words]
    return " ".join(words)

df["Resume_str"] = df["Resume_str"].apply(remove_stopwords)

def lemmatize_text(text):
    doc = nlp(text)
    return " ".join([token.lemma_ for token in doc])

df["Resume_str"] = df["Resume_str"].apply(lemmatize_text)

df.drop_duplicates(subset=["Resume_str"], inplace=True)

df.to_csv("cleaned_resume_dataset.csv", index=False)

print(df.head())
"""# **Dataset Augmented**"""
import random
import nltk
import pandas as pd
from nltk.corpus import wordnet
from gensim.models import Word2Vec
from transformers import pipeline

nltk.download('wordnet')
nltk.download('omw-1.4') paraphrase = pipeline("text2text-generation", model="humarin/chatgpt_paraphraser_on_T5_base")
def synonym_replacement(sentence, n=1):
    words = sentence.split()
    if not words:  
        return sentence
    new_words = words.copy()
    for _ in range(n):
        word = random.choice(words)
        synonyms = wordnet.synsets(word)
        if synonyms:
            synonym = synonyms[0].lemmas()[0].name()
            new_words[new_words.index(word)] = synonym
    return ' '.join(new_words)
    df["synonym_replacement"] = df["Resume_str"].apply(lambda x: synonym_replacement(x))
df.to_csv("augmented_resume_dataset.csv", index=False)
print(df.head())
"""# **Vectorization**"""
!pip install gensim
from gensim.models import Word2Vec

word2vec_model = Word2Vec(sentences=df["synonym_replacement"], vector_size=100, window=5, min_count=1, workers=4)

def get_word2vec_vector(tokens):
    vectors = [word2vec_model.wv[word] for word in tokens if word in word2vec_model.wv]
    return sum(vectors) / len(vectors) if vectors else [0] * 100  

df["word2vec_vector"] = df["synonym_replacement"].apply(get_word2vec_vector)
print(df.head())
from sklearn.feature_extraction.text import TfidfVectorizer

df["processed_text_tf-idf"] = df["synonym_replacement"].apply(lambda x: " ".join(x))

tfidf_vectorizer = TfidfVectorizer(max_features=5000)
tfidf_vectors = tfidf_vectorizer.fit_transform(df["synonym_replacement"])

tfidf_df = pd.DataFrame(tfidf_vectors.toarray(), columns=tfidf_vectorizer.get_feature_names_out())

df = pd.concat([df, tfidf_df], axis=1)
print(df.head())
df.to_csv("vectorized_resume_dataset.csv", index=False)

"""# **Pretesting with KNN**"""

from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer 
import pandas as pd

X = df.drop(columns=['Category', 'word2vec_vector']) 
y = df['Category']  
scaler = StandardScaler() 
encoder = LabelEncoder()
y = encoder.fit_transform(y) 
for column in X.select_dtypes(include=['object']).columns:
    X[column] = encoder.fit_transform(X[column])
    
imputer = SimpleImputer(strategy='mean') 
X = imputer.fit_transform(X) 
X_scaled = scaler.fit_transform(X)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

knn = KNeighborsClassifier(n_neighbors=5, metric='cosine')  

knn.fit(X_train, y_train)

y_pred = knn.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f"KNN Accuracy: {accuracy:.4f}")
print("Classification Report:\n", classification_report(y_test, y_pred))
